import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import re
from datetime import datetime
import os
import csv

print("="*60)
print("SCRAPING ELMINASSA.COM - VERSION FINALE")
print("="*60)

# Configuration du navigateur
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.add_argument('--disable-blink-features=AutomationControlled')
options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

print(" Lancement du navigateur...")
driver = webdriver.Chrome(options=options)

try:
    url = "https://www.elminassa.com/list"
    print(f"Chargement de {url}...")
    
    driver.get(url)
    time.sleep(5)
    
    # GÃ©rer la popup de localisation si elle apparaÃ®t
    try:
        for texte in ['Ø­Ø³Ù†Ø§', 'OK', 'Fermer']:
            try:
                btn = driver.find_element(By.XPATH, f"//button[contains(text(), '{texte}')]")
                btn.click()
                print(f"âœ… Popup fermÃ©e")
                time.sleep(2)
                break
            except:
                continue
    except:
        pass
    
    # CLIQUER SUR "ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø²ÙŠØ¯" (Charger plus) jusqu'Ã  Ã©puisement
    clics = 0
    while True:
        try:
            # Chercher le bouton de chargement
            load_more = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø²ÙŠØ¯')]"))
            )
            driver.execute_script("arguments[0].scrollIntoView(true);", load_more)
            time.sleep(1)
            load_more.click()
            clics += 1
            print(f" Clic {clics} - ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø²ÙŠØ¯")
            time.sleep(3)
        except:
            print(f"âœ… Plus de bouton aprÃ¨s {clics} clics")
            break
    
    # RÃ©cupÃ©rer le HTML final
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    # Trouver TOUS les conteneurs d'annonces (swiper-slide)
    # Chaque annonce est dans un div avec classe 'swiper-slide'
    annonces = soup.find_all('div', class_='swiper-slide')
    
    print(f"\nğŸ” {len(annonces)} annonces trouvÃ©es")
    
    donnees = []
    
    for i, annonce in enumerate(annonces):
        try:
            # ----- PRIX -----
            prix_elem = annonce.find('span', class_='myTopRight2')
            prix = prix_elem.text.strip() if prix_elem else "Non spÃ©cifiÃ©"
            
            # ----- TYPE DE BIEN (en arabe) -----
            type_elem = annonce.find('span', class_='myTopLeftt2')
            type_arabe = type_elem.text.strip() if type_elem else ""
            
            # Convertir en franÃ§ais
            type_bien = "Non spÃ©cifiÃ©"
            if 'Ù‚Ø·Ø¹Ø© Ø£Ø±Ø¶ÙŠØ©' in type_arabe:
                type_bien = 'Terrain'
            elif 'Ù…Ù†Ø²Ù„' in type_arabe:
                type_bien = 'Maison'
            elif 'Ø´Ù‚Ø©' in type_arabe:
                type_bien = 'Appartement'
            elif 'Ù…ÙƒØªØ¨' in type_arabe:
                type_bien = 'Bureau'
            elif 'Ù…Ø­Ù„ ØªØ¬Ø§Ø±ÙŠ' in type_arabe:
                type_bien = 'Local commercial'
            
            # ----- TITRE (description) -----
            # Le titre est dans le div avec dir="auto" lang="ar"
            titre_div = annonce.find_next('div', {'dir': 'auto', 'lang': 'ar'})
            titre = titre_div.text.strip() if titre_div else "Non spÃ©cifiÃ©"
            
            # ----- IMAGE -----
            img = annonce.find('img')
            image_url = img['src'] if img and img.has_attr('src') else "Non spÃ©cifiÃ©"
            
            # ----- URL DE L'ANNONCE (pas de lien direct, mais on peut utiliser l'image)
            # L'image est cliquable, mais le lien est dans un parent
            parent_link = annonce.find_parent('a')
            url_annonce = parent_link['href'] if parent_link and parent_link.has_attr('href') else "Non spÃ©cifiÃ©"
            if url_annonce.startswith('/'):
                url_annonce = "https://www.elminassa.com" + url_annonce
            
            # ----- EXTRAIRE LA SURFACE DU TITRE -----
            surface_m2 = "Non spÃ©cifiÃ©"
            surface_match = re.search(r'(\d+)\s*m[Â²2]', titre)
            if surface_match:
                surface_m2 = surface_match.group(1) + " mÂ²"
            
            # ----- EXTRAIRE LE QUARTIER DU TITRE -----
            quartier = "Non spÃ©cifiÃ©"
            quartiers = ['ØªÙØ±Øº Ø²ÙŠÙ†Ø©', 'Ø¯Ø§Ø± Ø§Ù„Ù†Ø¹ÙŠÙ…', 'Ù„ÙƒØµØ±', 'Ø§Ù„Ù…ÙŠÙ†Ø§Ø¡', 'Ø§Ù„Ø³Ø¨Ø®Ø©', 'ØªÙŠØ§Ø±Øª', 'Ø§Ù„Ø±ÙŠØ§Ø¶', 'Ø¹Ø±ÙØ§Øª', 'ØªÙˆØ¬Ù†ÙŠÙ†']
            for q in quartiers:
                if q in titre:
                    # Convertir en franÃ§ais
                    quartier_map = {
                        'ØªÙØ±Øº Ø²ÙŠÙ†Ø©': 'Tevragh Zeina',
                        'Ø¯Ø§Ø± Ø§Ù„Ù†Ø¹ÙŠÙ…': 'Dar Naim',
                        'Ù„ÙƒØµØ±': 'Ksar',
                        'Ø§Ù„Ù…ÙŠÙ†Ø§Ø¡': 'El Mina',
                        'Ø§Ù„Ø³Ø¨Ø®Ø©': 'Sebkha',
                        'ØªÙŠØ§Ø±Øª': 'Teyarett',
                        'Ø§Ù„Ø±ÙŠØ§Ø¶': 'Riyad',
                        'Ø¹Ø±ÙØ§Øª': 'Arafat',
                        'ØªÙˆØ¬Ù†ÙŠÙ†': 'Toujounine'
                    }
                    quartier = quartier_map.get(q, q)
                    break
            
            # CrÃ©er l'annonce
            annonce_data = {
                'source': 'elminassa.com',
                'titre': titre,
                'prix': prix,
                'type_bien': type_bien,
                'quartier': quartier,
                'surface_m2': surface_m2,
                'point_repere': 'Non spÃ©cifiÃ©',
                'vendeur': 'elminassa.com',
                'date_publication': 'Non spÃ©cifiÃ©e',
                'nb_images': '1',
                'image_url': image_url,
                'date_scraping': datetime.now().strftime('%Y-%m-%d'),
                'ville': 'Nouakchott',
                'nb_chambres': 'Non spÃ©cifiÃ©',
                'nb_sdb': 'Non spÃ©cifiÃ©',
                'description': titre,
                'id_unique': str(i+1),
                'url': url_annonce,
                'type_annonce': 'Non spÃ©cifiÃ©',
                'nb_vues': 'Non spÃ©cifiÃ©',
                'nb_pieces_total': 'Non spÃ©cifiÃ©',
                'meuble': 'Non spÃ©cifiÃ©',
            }
            
            donnees.append(annonce_data)
            print(f"  âœ… {titre[:50]}... - {prix}")
            
        except Exception as e:
            print(f"  âŒ Erreur annonce {i}: {e}")
            continue
    
    print(f"\nTotal annonces extraites: {len(donnees)}")
    
    # Sauvegarde
    if donnees:
        df = pd.DataFrame(donnees)
        df.to_csv('data_raw/elminassa.csv', index=False, encoding='utf-8-sig')
        print(f"DonnÃ©es sauvegardÃ©es dans data_raw/elminassa.csv")
        
        
    else:
        print("âŒ Aucune donnÃ©e extraite")
        
finally:
    driver.quit()
    print("\nğŸ‰ Scraping terminÃ©!")